Fix the following issues. The issues can be from different files or can overlap on same lines in one file.

- In @alembic/versions/4745fd629317_initial_schema_creation.py around lines 230 - 231, The migration currently assumes fixed constraint names when dropping/creating constraints (e.g., op.drop_constraint('research_tasks_user_id_fkey', 'research_tasks', ...) and the duplicate use for 'users_email_key'); update the migration to look up the existing constraint name dynamically or check for its existence before dropping to avoid failures on databases with different naming. Specifically, replace direct drops of 'research_tasks_user_id_fkey' and any hardcoded 'users_email_key' with a defensive lookup (using Inspector.get_foreign_keys / get_unique_constraints or conditional existence checks) for the constraint on table 'research_tasks' (column 'user_id') and the unique constraint on 'users' (column 'email'), then call op.drop_constraint/op.create_foreign_key only with the discovered name or skip the drop if not present, preserving the intended ondelete='CASCADE' behavior when calling op.create_foreign_key('fk_research_tasks_user_id', 'research_tasks', 'users', ['user_id'], ['id'], ondelete='CASCADE').

- In @alembic/versions/4745fd629317_initial_schema_creation.py around lines 153 - 157, The migration currently deletes orphaned rows from table 'agent_actions' using op.execute(...) before altering the column; instead, add a backup step that copies those rows to a durable backup (e.g., insert into a dedicated backup table or a temporary table with a migration run_id/timestamp) prior to deletion, then proceed with op.execute("DELETE FROM agent_actions WHERE task_id IS NULL") and op.alter_column('agent_actions','task_id',...); ensure the backup creation references the same filter (task_id IS NULL), includes all original columns and a created_at/run_id marker, and add a log entry or comment to indicate where backups are stored so rows can be recovered if needed.

- In @alembic/versions/4745fd629317_initial_schema_creation.py around lines 27 - 28, The current "Fresh DB Check" uses if 'users' not in tables and 'research_tasks' not in tables which misses partial states; update the logic to explicitly handle partial schemas by checking each table against the tables set (variables 'tables', table names 'users' and 'research_tasks') and if exactly one is present raise a clear error or abort the migration with a descriptive message about a partial schema state so we don't assume both exist; alternatively, if the intended behavior is to treat any missing table as fresh, change the condition to use or (i.e., if 'users' not in tables or 'research_tasks' not in tables) but prefer explicit validation and an early failure path for the mixed/partial case.

- In @alembic/versions/4745fd629317_initial_schema_creation.py around lines 167 - 168, The hardcoded drop_constraint call using 'agent_actions_task_id_fkey' can fail if PostgreSQL generated a different name; replace the single-line op.drop_constraint with logic that queries the DB for the existing foreign-key constraint on the agent_actions.task_id column (using op.get_bind() and a SELECT against information_schema.table_constraints / key_column_usage or pg_constraint), then drop it conditionally via op.execute("ALTER TABLE agent_actions DROP CONSTRAINT IF EXISTS <constraint_name>") when found, and then keep the existing op.create_foreign_key('fk_agent_actions_task_id', 'agent_actions', 'research_tasks', ['task_id'], ['id'], ondelete='CASCADE') to create the new named constraint.

- In @api/routes/research.py around lines 238 - 242, The route calls UUID(task_id) before ResearchService.get_research_task which raises ValueError for malformed IDs and currently bubbles up as a 500; catch that specific error and convert it into a 400 Bad Request by validating/parsing the UUID (or wrapping UUID(task_id) in a try/except ValueError) and raise HTTPException(status_code=400, detail="Invalid task_id format") when invalid, leaving ResearchService.get_research_task and the existing 404 handling unchanged.

- In @api/routes/research.py around lines 294 - 298, The endpoint currently calls ResearchService.get_research_task(db, UUID(task_id)) without validating task_id, which can raise on malformed UUIDs; update the handler to validate the UUID string before lookup by attempting to construct UUID(task_id) inside a try/except (or use a validator) and if it fails raise HTTPException(status_code=400, detail="Malformed UUID"), otherwise pass the validated UUID to ResearchService.get_research_task and keep the existing 404 check for missing tasks.

- In @database/connection.py around lines 46 - 49, The current logic on ASYNC_DATABASE_URL naively strips all query parameters when it detects "sslmode=", which removes other valid params; instead, parse the URL from ASYNC_DATABASE_URL, parse its query string, remove only the sslmode key (case-insensitive), and rebuild ASYNC_DATABASE_URL with the remaining query params preserved; update the block that currently checks "if 'sslmode=' in ASYNC_DATABASE_URL:" to use a proper URL/query parse and reconstruction routine so only the sslmode parameter is dropped while keeping application_name, connect_timeout, target_session_attrs, etc.

- In @database/connection.py at line 67, The current engine kwargs set connect_args using "connect_args=async_connect_args if async_connect_args else None", which converts an empty dict to None and can break SQLAlchemy; change this to always pass a dict (e.g., "connect_args=async_connect_args" so an empty {} is preserved) or build engine kwargs conditionally so "connect_args" is omitted only when truly undefined; update the usage of the symbol connect_args and the variable async_connect_args accordingly where the engine is created.

- In @database/connection.py around lines 42 - 44, The conditional that attempts to reassign ASYNC_DATABASE_URL is inverted/redundant and prevents the intended behavior; remove the if block and keep only the single assignment ASYNC_DATABASE_URL = DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://") so ASYNC_DATABASE_URL correctly contains the asyncpg scheme whether DATABASE_URL originally included it or not (refer to the ASYNC_DATABASE_URL and DATABASE_URL variables).

- In @orchestrator/standard_orchestrator.py around lines 156 - 158, The start_time assigned to the transient state before calling research_graph.ainvoke (execution_start / state.start_time) may not survive if ainvoke returns a new dict/object; after receiving the result and constructing final_state (ResearchState(...)), set final_state.start_time = execution_start (or copy from the returned dict) analogous to how end_time is set so the returned ResearchState always contains the start timestamp.

- In @services/research_service.py around lines 124 - 134, The code assigns final_state.dict() to task.metadata_json and then unconditionally overwrites it when metadata_json is present, causing loss of final_state; modify the assignment logic in the block around task.metadata_json so the two sources are combined or separated: either merge final_state.dict() into the existing metadata_json (e.g., deep-merge keys from final_state into metadata_json before assigning task.metadata_json) or store final_state in a dedicated field on task (e.g., task.final_state_json = final_state.dict()) and leave task.metadata_json = metadata_json untouched; adjust the branch that handles final_state, metadata_json, and task.error_message accordingly to ensure both pieces are preserved.

- In @tests/test_standard_research.py around lines 202 - 205, There are two identical assertions for the length of actions; drop the duplicate to avoid redundant checks — keep a single assert len(actions) == 3 and remove the second one after the for-loop that iterates over actions (the assertions and loop involving the actions variable in tests/test_standard_research.py).

- In @tests/test_standard_research.py around lines 46 - 50, The assertion for task.estimated_cost_usd is incorrect because 0.0 is falsy; update the test in tests/test_standard_research.py to explicitly check the expected numeric value (e.g., assert task.estimated_cost_usd == 0.0) or assert it is not None depending on intent — reference the test variable task and its estimated_cost_usd attribute when making the change.